# -*- coding: utf-8 -*-
"""prompt_statisical_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XEkLQ6O01DCIVpLYifpsIyUFixnIEfLl
"""

# ðŸ”¬ One-cell Prompt Sensitivity Analysis (STS-ready) â€” robust to column name variations
!pip -q install pingouin scikit-posthocs

import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns
import pingouin as pg
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

plt.rcParams["figure.dpi"] = 140
sns.set_context("talk")

# === 0) Load data ===
csv_path = "/content/normalized_dataset.csv"  # <-- e.g., "myco-prompt-quality-analysis - column_subset_analysis.csv"
df = pd.read_csv(csv_path)

# Identify numeric "score" columns; EXCLUDE a plain 'score' aggregate if present
score_cols = [
    c for c in df.columns
    if ("score" in c.lower() and df[c].dtype.kind in "if" and c.lower() != "score")
]
n_papers, n_prompts = df.shape[0], len(score_cols)
print(f"Detected {n_prompts} prompt columns across {n_papers} papers.")
print("Score columns:", score_cols)

# === 1) Exploratory distributions ===
plt.figure(figsize=(14,6))
sns.boxplot(data=df[score_cols])
plt.xticks(rotation=45); plt.ylabel("Score (1â€“10)")
plt.title("Distribution of Scores by Prompt Structure")
plt.tight_layout(); plt.show()

corr = df[score_cols].corr(method="pearson")
plt.figure(figsize=(20,20))
sns.heatmap(corr, annot=True, fmt=".2f", vmin=-1, vmax=1, cmap="coolwarm")
plt.title("Correlation Between Prompt Structures (Pearson)")
plt.tight_layout(); plt.show()

# === 2) Sensitivity per paper (variance, std, range) ===
paper_var = df[score_cols].var(axis=1, ddof=1)
paper_std = df[score_cols].std(axis=1, ddof=1)
paper_rng = df[score_cols].max(axis=1) - df[score_cols].min(axis=1)
sens_summary = pd.DataFrame({"variance": paper_var, "std": paper_std, "range": paper_rng})
print("\nPer-paper sensitivity summary:\n", sens_summary.describe())

plt.figure(figsize=(8,5))
plt.hist(paper_rng, bins=20, edgecolor="black")
plt.title("Distribution of Score Ranges Across Prompts (Per Paper)")
plt.xlabel("Range (max - min across prompts)"); plt.ylabel("Number of papers")
plt.tight_layout(); plt.show()

df[score_cols].std(axis=1, ddof=1).sort_values(ascending=False).head(10)

# === 3) Reliability: Cronbachâ€™s Alpha & ICC(2,1) ===
alpha, _ = pg.cronbach_alpha(df[score_cols])

# Long format for ICC & RM-ANOVA (avoid name collision with existing 'score')
long = df[score_cols].reset_index().melt(
    id_vars="index", var_name="prompt", value_name="score_value"
).rename(columns={"index":"paper"}).dropna()

icc_table = pg.intraclass_corr(data=long, targets="paper", raters="prompt", ratings="score_value")
# ICC(2,1): two-way random effects, absolute agreement, single measure
icc_21 = icc_table.loc[icc_table["Type"]=="ICC2","ICC"].values[0]
icc_ci  = icc_table.loc[icc_table["Type"]=="ICC2","CI95%"].values[0]

# === 4) Repeated-measures ANOVA + post-hoc with effect sizes ===
rm = pg.rm_anova(dv="score_value", within="prompt", subject="paper",
                 data=long, detailed=True, effsize="np2")

# Robustly pick the best available p-value column (prefer GG-corrected if present)
def pick_pval_row(row):
    # all columns that look like p-values
    pcols = [c for c in row.index if c.lower().startswith('p')]
    # preference order
    pref = ['p-gg-corr','p-gg','p-corr','p-unc','p']
    for name in pref:
        for c in pcols:
            if c.lower() == name:
                return float(row[c])
    return float(row[pcols[0]]) if pcols else float('nan')

prompt_mask = rm["Source"].astype(str).str.lower().eq("prompt")
prompt_row = rm.loc[prompt_mask].iloc[0]
anova_p = pick_pval_row(prompt_row)

# Effect size: prefer partial eta-squared 'np2', else try any eta-like column
if "np2" in rm.columns:
    anova_eta = float(prompt_row["np2"])
else:
    alt_eff = [c for c in rm.columns if ("eta" in c.lower() or "eps" in c.lower() or "eff" in c.lower())]
    anova_eta = float(prompt_row[alt_eff[0]]) if alt_eff else float("nan")

# Post-hoc paired tests
pw = pg.pairwise_ttests(dv="score_value", within="prompt", subject="paper",
                        data=long, padjust="bonf", effsize="cohen", return_desc=True)

# Build a robust "top_pairs" table with whatever columns exist
def first_available(cols, candidates):
    for c in candidates:
        if c in cols:
            return c
    return None

cols = pw.columns
col_A = first_available(cols, ["A","Cond1","cond1"])
col_B = first_available(cols, ["B","Cond2","cond2"])
col_p = first_available(cols, ["p-corr","p-unc","p-val","pval","p"])
col_d = first_available(cols, ["cohen-d","effsize","hedges","Cliff-d","r"])
col_mA = first_available(cols, ["mean(A)","Mean(A)","mean_A","mean1"])
col_mB = first_available(cols, ["mean(B)","Mean(B)","mean_B","mean2"])

keep = [c for c in [col_A,col_B,col_p,col_d,col_mA,col_mB] if c is not None]
top_pairs = pw[keep].sort_values(col_p).head(5)

# === 5) PCA: prompt families / structure ===
X = df[score_cols].astype(float).fillna(df[score_cols].mean())
Z = StandardScaler().fit_transform(X)
pca = PCA(n_components=2, random_state=0).fit(Z)
PC = pca.transform(Z)
expl = pca.explained_variance_ratio_

plt.figure(figsize=(6.5,5.5))
plt.scatter(PC[:,0], PC[:,1], s=8, alpha=0.3)
plt.xlabel("PC1"); plt.ylabel("PC2"); plt.title("Papers in Prompt-Score PCA space")
plt.tight_layout(); plt.show()

loadings = pd.DataFrame({"PC1": pca.components_[0], "PC2": pca.components_[1]}, index=score_cols)\
            .sort_values("PC1", ascending=False)

# === 6) Helpful descriptive tables ===
prompt_means = df[score_cols].mean().sort_values(ascending=False).rename("mean")
prompt_stds  = df[score_cols].std().rename("std")
prompt_desc  = pd.concat([prompt_means, prompt_stds], axis=1)
print("\nPrompt-level means & std (prompt-only; 'score' excluded):\n", prompt_desc)

# === 7) Final concise summary printout (ready for report) ===
median_range = float(np.median(paper_rng))
mean_range   = float(np.mean(paper_rng))
prop_high_rng = float((paper_rng >= 6).mean())  # share with very high prompt sensitivity

print("\n" + "="*72)
print("SUMMARY (paste into report; customize wording as needed)")
print("="*72)
print(f"Papers (N): {n_papers} | Prompts (k): {n_prompts}")
print(f"Internal consistency: Cronbachâ€™s Î± = {alpha:.3f}")
print(f"Absolute agreement: ICC(2,1) = {icc_21:.3f}  (95% CI {icc_ci})")
print(f"RM-ANOVA: prompts differ in mean score, p = {anova_p:.3e}, partial Î·Â² = {anova_eta:.3f}")
print(f"Sensitivity (per-paper across prompts): median range = {median_range:.2f}, mean range = {mean_range:.2f}")
print(f"High-sensitivity papers (range â‰¥ 6): {prop_high_rng*100:.1f}%")
print(f"PCA: PC1 = {expl[0]:.2%} variance, PC2 = {expl[1]:.2%} variance")
print("\nTop post-hoc differences (Bonferroni-corrected):")
print(top_pairs.to_string(index=False))
print("\nPrompt families (PCA loadings on PC1, top to bottom):")
print(loadings.head(5).to_string())
print("="*72)

# (Optional) also display full tables nicely in Colab
import IPython
IPython.display.display(icc_table)
IPython.display.display(rm)
IPython.display.display(pw.sort_values(col_p).head(15))
IPython.display.display(loadings)