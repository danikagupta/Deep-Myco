# -*- coding: utf-8 -*-
"""PDF_Extract_intra-prompt.ipynb

Automatically generated by Colab.

"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# --- Install Dependencies ---
!pip install -q pandas numpy scipy scikit-learn sentence-transformers rapidfuzz

# ============================================
# üìä Intra-Prompt Variability Analysis (Fixed + Robust Version)
# ============================================


# --- Imports ---
import os
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from scipy.stats import variation, entropy
from rapidfuzz import fuzz

# --- CONFIGURATION ---
BASE_PATH = "/content/drive/MyDrive/LLM_extractions"   # ‚¨ÖÔ∏è change this if your folder differs

DIRS = ["1", "2", "3"]
OUTPUT_CSV = os.path.join(BASE_PATH, "intra_prompt_metrics.csv")

# --- Load sentence embedding model ---
model = SentenceTransformer("all-MiniLM-L6-v2")

# --- Helper Functions ---
def clean_headers(df):
    """Normalize header names across runs."""
    df.columns = (
        df.columns
        .str.replace('"', '', regex=False)
        .str.strip()
        .str.lower()
        .str.replace('mycellium', 'mycelium', regex=False)
    )
    return df

def load_paper_versions(paper_name):
    """Load three extraction files (one per run)."""
    dfs = []
    for d in DIRS:
        path = os.path.join(BASE_PATH, d, paper_name)
        if os.path.exists(path):
            try:
                df = pd.read_csv(path, sep=",", engine="python")
            except Exception:
                try:
                    df = pd.read_table(path, engine="python")
                except Exception:
                    print(f"‚ö†Ô∏è Skipping {paper_name} in {d} ‚Äî unreadable file")
                    return None
            df = clean_headers(df)
            dfs.append(df)
    # Keep only common columns across all runs
    if len(dfs) == 3 and all(len(df.columns) > 0 for df in dfs):
        common_cols = set.intersection(*[set(df.columns) for df in dfs])
        dfs = [df[list(common_cols)] for df in dfs]
        return dfs if len(common_cols) > 0 else None
    return None

def textify_df(df):
    """Flatten dataframe rows to single text lines."""
    df = df.fillna("").astype(str)
    return [" | ".join(row) for _, row in df.iterrows()]

def jaccard_similarity(a, b):
    a, b = set(a.lower().split()), set(b.lower().split())
    return len(a & b) / len(a | b) if a | b else 1.0

def average_pairwise_metric(items, func):
    vals = []
    for i in range(len(items)):
        for j in range(i + 1, len(items)):
            vals.append(func(items[i], items[j]))
    return np.mean(vals) if vals else np.nan

def compute_entropy(series):
    values = series.value_counts(normalize=True)
    return entropy(values, base=2)

def fuzzy_row_match(df1, df2, threshold=90):
    """Approximate structural overlap between two tables."""
    df1_rows = [" | ".join(map(str, r)) for r in df1.values]
    df2_rows = [" | ".join(map(str, r)) for r in df2.values]
    matches = 0
    for r1 in df1_rows:
        if any(fuzz.ratio(r1, r2) > threshold for r2 in df2_rows):
            matches += 1
    return matches / len(df1_rows) if df1_rows else 0

# --- Core Metric Computation ---
def analyze_paper(paper_name):
    dfs = load_paper_versions(paper_name)
    if dfs is None:
        return None

    try:
        # Semantic similarity (cosine)
        texts = [textify_df(df) for df in dfs]
        embeddings = [model.encode(t, convert_to_tensor=True).mean(0).cpu().numpy() for t in texts]
        cos_vals = [cosine_similarity([embeddings[i]], [embeddings[j]])[0, 0]
                    for i in range(3) for j in range(i + 1, 3)]
        mean_cosine = np.mean(cos_vals)

        # Lexical similarity (Jaccard)
        combined_texts = [" ".join(t) for t in texts]
        mean_jaccard = average_pairwise_metric(combined_texts, jaccard_similarity)

        # Entropy (diversity)
        entropies = []
        for col in dfs[0].columns:
            vals = pd.concat([df[col].astype(str) for df in dfs])
            entropies.append(compute_entropy(vals))
        mean_entropy = np.nanmean(entropies)

        # Numeric stability (Coefficient of Variation)
        cvs = []
        for col in dfs[0].columns:
            try:
                all_vals = pd.concat([pd.to_numeric(df[col], errors='coerce') for df in dfs])
                all_vals = all_vals.dropna()
                if len(all_vals) > 1 and all_vals.std() > 0:
                    cvs.append(variation(all_vals))
            except Exception:
                continue
        num_cv_mean = np.nanmean(cvs) if cvs else np.nan

        # Row match (fuzzy structural consistency)
        row_match_rates = [fuzzy_row_match(dfs[i], dfs[j]) for i in range(3) for j in range(i + 1, 3)]
        row_match_rate = np.nanmean(row_match_rates)

        return {
            "paper": paper_name,
            "mean_cosine": mean_cosine,
            "mean_jaccard": mean_jaccard,
            "mean_entropy": mean_entropy,
            "num_cv_mean": num_cv_mean,
            "row_match_rate": row_match_rate,
        }

    except Exception as e:
        print(f"‚ö†Ô∏è Error processing {paper_name}: {e}")
        return None

# --- Run Across All Papers ---
all_papers = sorted(set(os.listdir(os.path.join(BASE_PATH, "1"))))
results = []

for paper in all_papers:
    metrics = analyze_paper(paper)
    if metrics:
        results.append(metrics)

df_results = pd.DataFrame(results)
df_results.to_csv(OUTPUT_CSV, index=False)

print(f"‚úÖ Intra-prompt analysis complete. Results saved to:\n{OUTPUT_CSV}")
display(df_results.head())