# -*- coding: utf-8 -*-
"""intra-prompt-variability-stats.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jb_8LMH7BQJZnpPzTWQshegB442sBQBq
"""

#
# ------------------------------------------------------------------
# What this cell does:
# 1) Loads your *normalized* CSV (0–1 scores) with 3 runs each for:
#    - baseline_*_score
#    - few_shot_*_score
#    - chain_of_thought_*_score
# 2) Computes per-file mean/std for each prompt, plus aggregated stats
# 3) Runs ANOVA + Cohen’s d between prompt types
# 4) Correlations (score↔variability and cross-prompt variability)
# 5) Plots histograms, boxplots, and mean-vs-std scatter plots
# 6) Lists 95th-percentile “unstable” papers per prompt and their intersection
# 7) Saves per-file stats to CSV
#
# Notes:
# - Uses matplotlib only, one chart per figure, default colors.
# - If your file is elsewhere, set CSV_PATH below.
# ------------------------------------------------------------------

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# ---- Config ----
CSV_PATH = "/content/newexperiments_prompt_sensitity_normalized.csv"  # change if needed
PROMPTS = {
    "baseline": "baseline_newexpt",
    "few_shot": "few_shot_newexpt",
    "chain_of_thought": "cot_newexpt",
}
SCORE_SUFFIX = "_score"

# ---- Load ----
df = pd.read_csv(CSV_PATH)

# ---- Identify score columns per prompt ----
def cols_for(prefix: str) -> list:
    return [c for c in df.columns if c.startswith(prefix) and c.endswith(SCORE_SUFFIX)]

cols = {name: cols_for(prefix) for name, prefix in PROMPTS.items()}

# Sanity check & echo columns
for k, v in cols.items():
    if len(v) == 0:
        raise ValueError(f"No score columns found for prompt: {k}")
    print(f"{k}: {v}")

# ---- Per-file (row-wise) stats: mean & std across 3 runs for each prompt ----
per_file_means = pd.DataFrame({f"{k}_mean": df[v].mean(axis=1) for k, v in cols.items()})
per_file_stds  = pd.DataFrame({f"{k}_std":  df[v].std(axis=1)  for k, v in cols.items()})

id_like_cols = [c for c in ["id", "filename", "title"] if c in df.columns]
per_file_stats = (
    pd.concat([df[id_like_cols], per_file_means, per_file_stds], axis=1)
    if id_like_cols else pd.concat([per_file_means, per_file_stds], axis=1)
)

# ---- Aggregate intra-prompt metrics across all files ----
def aggregate_stats(columns: list) -> dict:
    row_means = df[columns].mean(axis=1)
    row_stds  = df[columns].std(axis=1)
    row_vars  = df[columns].var(axis=1)
    return {
        "mean_of_means":  row_means.mean(),
        "median_of_means":row_means.median(),
        "mean_std_dev":   row_stds.mean(),
        "median_std_dev": row_stds.median(),
        "p90_std_dev":    row_stds.quantile(0.90),
        "p95_std_dev":    row_stds.quantile(0.95),
        "mean_variance":  row_vars.mean(),
        "min_score":      df[columns].min().min(),
        "max_score":      df[columns].max().max(),
        "total_evals":    int(df[columns].count().sum()),
        "num_files":      int(len(df)),
    }

agg = pd.DataFrame({k: aggregate_stats(v) for k, v in cols.items()}).T
print("\n=== Intra-prompt summary (normalized 0–1) ===")
print(agg.round(4))

# ---- ANOVA across prompt types (flattened vectors) ----
flattened = {k: pd.Series(df[v].values.flatten()).dropna() for k, v in cols.items()}
anova = stats.f_oneway(*flattened.values())
print("\n=== One-way ANOVA across prompt types (flattened normalized scores) ===")
print(f"F = {anova.statistic:.3f}, p = {anova.pvalue:.3e}")

# ---- Cohen's d for each pair ----
def cohens_d(x: np.ndarray, y: np.ndarray) -> float:
    x = np.asarray(x); y = np.asarray(y)
    nx, ny = len(x), len(y)
    sx, sy = x.std(ddof=1), y.std(ddof=1)
    pooled = np.sqrt(((nx-1)*sx**2 + (ny-1)*sy**2) / (nx + ny - 2))
    return (x.mean() - y.mean()) / pooled

pairs = [("baseline", "few_shot"), ("baseline", "chain_of_thought"), ("few_shot", "chain_of_thought")]
print("\n=== Cohen's d (pairwise) ===")
for a, b in pairs:
    dval = cohens_d(flattened[a], flattened[b])
    print(f"{a} vs {b}: d = {dval:.3f}")

# ---- Correlations: mean score vs std deviation (per file) ----
print("\n=== Correlation: per-file mean score vs per-file std (within prompt) ===")
for k in cols.keys():
    r = per_file_stats[f"{k}_mean"].corr(per_file_stats[f"{k}_std"])
    print(f"{k}: r = {r:.3f}")

# ---- Cross-prompt correlations: do the same files vary across prompts? ----
print("\n=== Correlation: per-file std between prompts ===")
for a, b in pairs:
    r = per_file_stats[f"{a}_std"].corr(per_file_stats[f"{b}_std"])
    print(f"{a} vs {b}: r = {r:.3f}")

# ---- Plotting ----
# 1) Histograms of per-file std for each prompt
for k in cols.keys():
    plt.figure()
    plt.hist(per_file_stats[f"{k}_std"].dropna(), bins=30, alpha=0.7)
    plt.title(f"{k} — per-file std distribution")
    plt.xlabel("Std deviation across 3 runs")
    plt.ylabel("Count")
    plt.tight_layout()
    plt.show()

# 2) Boxplots of per-file std for each prompt (separate figures)
for k in cols.keys():
    plt.figure()
    plt.boxplot(per_file_stats[f"{k}_std"].dropna(), vert=True, labels=[k])
    plt.title(f"{k} — per-file std boxplot")
    plt.ylabel("Std deviation across 3 runs")
    plt.tight_layout()
    plt.show()

# 3) Scatter plots: mean vs std for each prompt
for k in cols.keys():
    plt.figure()
    plt.scatter(per_file_stats[f"{k}_mean"], per_file_stats[f"{k}_std"], s=8, alpha=0.5)
    plt.title(f"{k} — mean vs std (per file)")
    plt.xlabel("Per-file mean score (normalized)")
    plt.ylabel("Per-file std deviation (normalized)")
    plt.tight_layout()
    plt.show()

# 4) Bar chart of mean std dev across prompts
plt.figure()
mean_stds = [per_file_stats[f"{k}_std"].mean() for k in cols.keys()]
plt.bar(list(cols.keys()), mean_stds)
plt.title("Average per-file std deviation by prompt")
plt.ylabel("Mean std deviation (normalized)")
plt.tight_layout()
plt.show()

# ---- Top unstable papers (>=95th percentile per prompt) + intersection ----
top_sets = {}
for k in cols.keys():
    thresh = per_file_stats[f"{k}_std"].quantile(0.95)
    top_sets[k] = per_file_stats[per_file_stats[f"{k}_std"] >= thresh]

key_col = "filename" if "filename" in per_file_stats.columns else (id_like_cols[0] if id_like_cols else None)

print("\n=== Top unstable papers (per prompt, >= 95th percentile of std) ===")
for k, dfk in top_sets.items():
    print(f"\n[{k}] count: {len(dfk)}")
    if key_col:
        print(
            dfk[[key_col, f"{k}_std"]]
            .sort_values(by=f"{k}_std", ascending=False)
            .head(10)
            .to_string(index=False)
        )

# Intersection across prompts
intersection = None
if key_col:
    sets = [set(top_sets[k][key_col]) for k in cols.keys()]
    intersection = set.intersection(*sets) if sets else set()
    print(f"\n=== Papers unstable across ALL prompts (intersection) — count: {len(intersection)} ===")
    print(list(intersection)[:20] if len(intersection) > 0 else "None found.")

# ---- Save per-file stats ----
out_path = "dye_per_file_prompt_variation.csv"
per_file_stats.to_csv(out_path, index=False)
print(f"\nSaved per-file stats to: {out_path}")