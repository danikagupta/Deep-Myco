# -*- coding: utf-8 -*-
"""icc-myco-promptstats.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11QOey5fkjyl6xKeSk1HRaPZYgExwzjwd
"""

# === Inter-rater reliability toolkit for Colab ===
# Computes: Krippendorff's alpha (interval), ICC(2,1), ICC(2,k), Kendall's W
# Usage:
# 1) Run this cell
# 2) Upload your CSV when prompted (or set CSV_PATH below)
# 3) Review the printed summary

!pip -q install numpy pandas scipy

import io
import numpy as np
import pandas as pd
from scipy.stats import rankdata
from typing import Tuple, Optional, List

# --- 1) Load data ---
CSV_PATH = '/content/experiments_prompt_analysis_avgs.csv'  # <- If you want to hardcode a path, put it here (e.g., "/content/data.csv")

def load_csv():
    if CSV_PATH:
        return pd.read_csv(CSV_PATH)
    try:
        from google.colab import files
        uploaded = files.upload()
        # Pick the first uploaded file
        fname = next(iter(uploaded.keys()))
        return pd.read_csv(io.BytesIO(uploaded[fname]))
    except Exception as e:
        raise RuntimeError(
            "Upload a CSV or set CSV_PATH. "
            f"Original error: {e}"
        )

df = load_csv()
print("Loaded shape:", df.shape)
print("Columns:", list(df.columns))

# --- 2) Choose columns ---
# Auto-detect rater columns as those ending with '_score' or named exactly 'score'.
def detect_rater_columns(frame: pd.DataFrame) -> List[str]:
    cols = []
    for c in frame.columns:
        c_low = str(c).strip().lower()
        if c_low.endswith("_avg") or c_low == "avg":
            # ensure numeric
            try:
                pd.to_numeric(frame[c])
                cols.append(c)
            except Exception:
                pass
    return cols

RATER_COLS = detect_rater_columns(df)
if not RATER_COLS:
    raise ValueError("No rater columns detected. Expect columns named 'score' or ending with '_score' and numeric.")

print("\nDetected rater columns:", RATER_COLS)

# Optional: auto-detect a grouping column (e.g., 'prompt', 'topic', 'task').
def detect_group_column(frame: pd.DataFrame) -> Optional[str]:
    candidates = ["prompt","topic","task","question","group","condition"]
    cols_lower = [c.lower() for c in frame.columns]
    for cand in candidates:
        if cand in cols_lower:
            # return original casing from df
            idx = cols_lower.index(cand)
            return frame.columns[idx]
    return None

GROUP_COL = detect_group_column(df)  # Change to a column name or None if you want to force/use a specific grouping

# --- 3) Reliability metrics ---

def krippendorff_alpha_interval(data: np.ndarray) -> float:
    """
    Krippendorff's alpha for interval data.
    Accepts an array of shape (n_items, k_raters) with NaNs allowed.
    Formula follows the 'difference variance' approach:
        alpha = 1 - Do/De
        Do: sum over items of squared deviations from each item's mean (over observed ratings)
        De: squared deviations from grand mean (over all observed ratings)
    """
    x = np.array(data, dtype=float)
    mask = ~np.isnan(x)
    # Observed disagreement (Do)
    Do = 0.0
    for i in range(x.shape[0]):
        row = x[i, :]
        m = np.sum(mask[i, :])
        if m <= 1:
            continue
        mu_i = np.nanmean(row)
        Do += np.nansum((row[mask[i, :]] - mu_i)**2)

    # Expected disagreement (De)
    all_vals = x[mask]
    if all_vals.size == 0:
        return np.nan
    grand_mean = np.nanmean(all_vals)
    De = np.nansum((all_vals - grand_mean)**2)

    if De == 0:
        return np.nan
    return 1.0 - (Do / De)

def kendalls_w(data: np.ndarray) -> float:
    """
    Kendall's coefficient of concordance W for multiple raters.
    Drops rows with any NaN (complete-case for W).
    Uses average ranks per rater; handles ties via average ranks.
    """
    x = np.array(data, dtype=float)
    # complete cases only
    x = x[~np.isnan(x).any(axis=1)]
    if x.shape[0] < 2 or x.shape[1] < 2:
        return np.nan
    n, k = x.shape

    # rank each rater's column across items
    ranks = np.zeros_like(x, dtype=float)
    for j in range(k):
        ranks[:, j] = rankdata(x[:, j], method="average")  # 1..n with average ranks for ties

    R = ranks.sum(axis=1)
    R_bar = R.mean()
    S = np.sum((R - R_bar) ** 2)
    # W formula (no extra tie-correction beyond average ranks; standard form)
    W = 12 * S / (k**2 * (n**3 - n))
    return float(W)

def icc_two_way_random_absolute(data: np.ndarray) -> Tuple[float, float]:
    """
    ICC(2,1) and ICC(2,k): Two-way random effects, absolute agreement
    McGraw & Wong (1996) / Shrout & Fleiss (1979) style ANOVA decomposition.

    Drops rows with any NaN (complete-case for ICC).
    Returns (ICC2_1, ICC2_k).
    """
    x = np.array(data, dtype=float)
    x = x[~np.isnan(x).any(axis=1)]
    n, k = x.shape if x.ndim == 2 and x.size else (0, 0)
    if n < 2 or k < 2:
        return (np.nan, np.nan)

    # Means
    mean_per_target = x.mean(axis=1, keepdims=True)   # n x 1
    mean_per_rater = x.mean(axis=0, keepdims=True)    # 1 x k
    grand_mean = x.mean()

    # Sum of squares
    SSR = n * np.sum((mean_per_rater - grand_mean) ** 2)  # Between raters
    SST = k * np.sum((mean_per_target - grand_mean) ** 2) # Between targets
    SSE = np.sum((x - mean_per_target - mean_per_rater + grand_mean) ** 2)  # Residual

    # Degrees of freedom
    df_r = k - 1
    df_t = n - 1
    df_e = (k - 1) * (n - 1)

    # Mean squares
    MSC = SSR / df_r if df_r > 0 else np.nan    # Raters
    MSR = SST / df_t if df_t > 0 else np.nan    # Targets
    MSE = SSE / df_e if df_e > 0 else np.nan    # Error

    # ICC(2,1) and ICC(2,k) for absolute agreement
    icc2_1 = (MSR - MSE) / (MSR + (k - 1) * MSE + (k * (MSC - MSE)) / n)
    icc2_k = (MSR - MSE) / (MSR + (MSC - MSE) / n)

    return float(icc2_1), float(icc2_k)

# --- 4) Compute metrics on the full matrix ---
X = df[RATER_COLS].apply(pd.to_numeric, errors="coerce").to_numpy()

alpha_interval = krippendorff_alpha_interval(X)
W = kendalls_w(X)
icc2_1, icc2_k = icc_two_way_random_absolute(X)

# For transparency, note the effective item/raters used for each metric
def complete_cases(mat):
    m = np.array(mat, dtype=float)
    return m[~np.isnan(m).any(axis=1)]

cc = complete_cases(X)

print("\n=== Inter-Rater Reliability Summary (All detected rater columns) ===")
print(f"Raters used: {len(RATER_COLS)} -> {RATER_COLS}")
print(f"Items (rows) in data: {X.shape[0]}")
print(f"Items used for Kendall's W / ICC (complete cases): {cc.shape[0]}")
print(f"Krippendorff's α (interval): {alpha_interval:.4f}")
print(f"Kendall's W (ordinal concordance): {W:.4f}")
print(f"ICC(2,1)  [single rater, absolute agreement]: {icc2_1:.4f}")
print(f"ICC(2,k)  [mean of k raters, absolute agreement]: {icc2_k:.4f}")

# --- 5) Optional: per-group breakdown if a grouping column exists ---
if GROUP_COL is not None and GROUP_COL in df.columns:
    print(f"\n=== Per-group reliability by '{GROUP_COL}' ===")
    for g, sub in df.groupby(GROUP_COL):
        Xg = sub[RATER_COLS].apply(pd.to_numeric, errors="coerce").to_numpy()
        alpha_g = krippendorff_alpha_interval(Xg)
        Wg = kendalls_w(Xg)
        icc2_1_g, icc2_k_g = icc_two_way_random_absolute(Xg)
        ccg = complete_cases(Xg)
        print(f"\nGroup: {g}  (n_items={Xg.shape[0]}, complete_cases={ccg.shape[0]})")
        print(f"  α_interval = {alpha_g:.4f}   W = {Wg:.4f}   ICC(2,1) = {icc2_1_g:.4f}   ICC(2,k) = {icc2_k_g:.4f}")

print("\nDone.")