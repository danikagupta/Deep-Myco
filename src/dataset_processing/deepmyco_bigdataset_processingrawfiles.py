# -*- coding: utf-8 -*-
"""DeepMyco-BigDataset-ProcessingRawFiles.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cypq1PpLWZq3dZ5t7QzkV1fawHjoI11Y
"""

# 1. Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')


# 2. Imports
import os
import glob
import pandas as pd
import matplotlib.pyplot as plt

# 3. Set the folder path in your Drive
#    üëâ Replace this with the actual path to your folder
#    Example: "/content/drive/MyDrive/data/csv_files"
folder_path = "/content/drive/Shareddrives/Danika-work4/2025/2025_11_16/pdf_tables_2025v4_csv"

# 4. Find all CSV files in the folder
csv_files = glob.glob(os.path.join(folder_path, "*.csv"))

print(f"Found {len(csv_files)} CSV file(s) in {folder_path}")

total_rows = 0

# 5. Loop over CSVs and count rows
for csv_file in csv_files:
    rows_in_file = 0

    # Use chunks so it works even for large files
    for chunk in pd.read_csv(csv_file, chunksize=100_000):
        rows_in_file += len(chunk)

    # rows_in_file currently EXCLUDES header row (header is not counted in chunks)
    total_rows += rows_in_file

    print(f"{os.path.basename(csv_file)}: {rows_in_file} row(s)")

print("-" * 40)
print(f"Total CSV files: {len(csv_files)}")
print(f"Total rows across all CSVs (excluding headers): {total_rows}")

# 4. Find all CSV files in the folder
csv_files = glob.glob(os.path.join(folder_path, "*.csv"))

print(f"Found {len(csv_files)} CSV file(s) in {folder_path}")

total_rows = 0
row_counts = []        # rows per file (excluding header)
file_names = []        # keep file names aligned with row_counts

# 5. Loop over CSVs and count rows
for csv_file in csv_files:
    rows_in_file = 0

    # Use chunks so it works even for large files
    for chunk in pd.read_csv(csv_file, chunksize=100_000):
        rows_in_file += len(chunk)

    total_rows += rows_in_file
    row_counts.append(rows_in_file)
    file_names.append(os.path.basename(csv_file))

    print(f"{os.path.basename(csv_file)}: {rows_in_file} row(s)")

print("-" * 40)
print(f"Total CSV files: {len(csv_files)}")
print(f"Total rows across all CSVs (excluding headers): {total_rows}")

# 6. Build a DataFrame of rows per file
df_counts = pd.DataFrame({
    "file": file_names,
    "rows": row_counts
})

print("\nRows per file:")
print(df_counts)

# 7. Basic statistics on the distribution
print("\nStatistics on rows per file (excluding headers):")
print(df_counts["rows"].describe())

# 8. Optional: include headers in total row count
total_rows_including_headers = total_rows + len(csv_files)
print(f"\nTotal rows including one header per file: {total_rows_including_headers}")

# 9. Plot a histogram of the distribution of rows per file
plt.figure(figsize=(8, 5))
plt.hist(df_counts["rows"], bins=20)
plt.xlabel("Rows per file (excluding header)")
plt.ylabel("Number of files")
plt.title("Distribution of rows per CSV file")
plt.show()

# 10. Combine all CSVs into one and save

# üëâ Name of the output file (change if you like)
output_path = os.path.join(folder_path, "combined_all_files.csv")

# Read and concatenate all CSVs
dfs = []
for csv_file in csv_files:
    df = pd.read_csv(csv_file)

    # Optional: keep track of which file each row came from
    df["source_file"] = os.path.basename(csv_file)

    dfs.append(df)

# Concatenate into one big DataFrame
combined_df = pd.concat(dfs, ignore_index=True)

# Save to a single CSV
combined_df.to_csv(output_path, index=False)

print(f"\n‚úÖ Combined CSV saved to: {output_path}")
print(f"Total rows in combined file: {len(combined_df)}")

import pandas as pd
import numpy as np
import os

# --- 1. Load the combined file ---
folder_path = "/content/drive/Shareddrives/Danika-work4/Textile"  # ‚Üê change this
input_path = os.path.join(folder_path, "DeepMyco_BigDataset_combined_all_files.csv")

df = pd.read_csv(input_path)

print("Original columns:")
print(list(df.columns))
print(f"\nOriginal column count: {df.shape[1]}")

# --- 2. Define coalescing groups: canonical_name -> list of equivalent columns ---
coalesce_groups = {
    # 1. Dye type
    "type_of_dye": [
        "type of dye",
        "type_of_dye",
    ],

    # 2. Fungi type (species/name)
    "type_of_fungi": [
        "type of fungi",
        "type_of_fungi",
    ],

    # 3. Dye concentration
    "concentration_of_dye": [
        "concentration of dye",
        "concentration_of_dye",
    ],

    # 4. Fungi concentration
    "concentration_of_fungi": [
        "concentration of fungi",
        "concentration_of_fungi",
    ],

    # 5. Method of agitation
    "method_of_agitation": [
        "method of agitation",
        "method_of_agitation",
    ],

    # 6. Decolorization result
    "decolorization_result": [
        "decolorization result",
        "decolorization_result",
    ],

    # 7. Additional info / notes
    "additional_information": [
        "any additional information",
        "additional information",
        "further_information",
    ],

    # 8. Fungal form (mycelium vs mass vs whole fungi)
    "fungal_form": [
        "whether the fungi was mycelium, fungal mass or whole fungi",
        "whether the fungi was mycellium, fungal mass or whole fungi",
        "whether fungi was mycellium, fungal mass or whole fungi",
        "type of fungi (mycelium, fungal mass or whole fungi)",
        "whether the fungi was mycellium, fungal mass, or whole fungi",
    ],
}

# --- 3. Helper: check conflicts and coalesce ---
def coalesce_columns(df, canonical_name, candidates):
    # Only keep columns that actually exist in the dataframe
    present = [c for c in candidates if c in df.columns]
    if len(present) == 0:
        print(f"[{canonical_name}] No candidate columns found in df, skipping.")
        return df

    # Check for conflicting rows: more than one non-null in the group
    # (Only NA is treated as missing; empty strings count as values here.)
    mask_conflict = df[present].notna().sum(axis=1) > 1
    n_conflict = mask_conflict.sum()

    if n_conflict > 0:
        print(f"\nERROR: {n_conflict} row(s) have multiple values in coalesced group '{canonical_name}'.")
        print("Here are the first few problematic rows (only showing this group's columns):")
        print(df.loc[mask_conflict, present].head())
        raise ValueError(f"Coalescing aborted due to conflicts in group '{canonical_name}'.")

    # Coalesce values across the group: first non-null from left to right
    df[canonical_name] = df[present].bfill(axis=1).iloc[:, 0]

    # Drop the non-canonical original columns (keep only the canonical name)
    to_drop = [c for c in present if c != canonical_name]
    df = df.drop(columns=to_drop)

    print(f"[{canonical_name}] Coalesced from columns: {present} -> kept as '{canonical_name}'")
    return df

# --- 4. Apply coalescing to all groups ---
for canonical, candidates in coalesce_groups.items():
    df = coalesce_columns(df, canonical, candidates)

print("\nNew columns after coalescing:")
print(list(df.columns))
print(f"\nNew column count: {df.shape[1]}")

# --- 5. (Optional) Save cleaned file ---
output_path = os.path.join(folder_path, "DeepMyco_BigDataset_coalesced.csv")
df.to_csv(output_path, index=False)
print(f"\n‚úÖ Coalesced dataset saved to: {output_path}")

import pandas as pd
import os

# --- 1. Paths: adjust folder_path if needed ---
folder_path = "/content/drive/Shareddrives/Danika-work4/Textile"  # ‚Üê change this

input_path = os.path.join(folder_path, "DeepMyco_BigDataset_coalesced.csv")
output_path = os.path.join(folder_path, "DeepMyco_BigDataset_fungal_merged.csv")

# --- 2. Load the coalesced dataset (from the previous step) ---
df = pd.read_csv(input_path)

print("Columns before fungal_form merge:")
print(list(df.columns))
print(f"Column count: {df.shape[1]}")

# --- 3. Define all columns that encode 'mycelium / fungal mass / whole fungi' in any way ---
fungal_form_candidates = [
    "fungal_form",  # may already exist from earlier coalescing
    "whether the fungi was mycelium, fungal mass or whole fungi",
    "whether the fungi was mycellium, fungal mass or whole fungi",
    "whether fungi was mycellium, fungal mass or whole fungi",
    "type of fungi (mycelium, fungal mass or whole fungi)",
    "whether the fungi was mycellium, fungal mass, or whole fungi",
    "type of fungi classification",  # <- NEW: the classification column to be merged
]

# Keep only those that actually exist in the dataframe
present = [c for c in fungal_form_candidates if c in df.columns]

print("\nFungal form columns present in df:")
print(present)

if len(present) == 0:
    raise ValueError("No fungal-form-related columns found in the dataframe.")

# --- 4. Check for conflicts: rows with >1 non-null value across these columns ---
mask_conflict = df[present].notna().sum(axis=1) > 1
n_conflict = mask_conflict.sum()

if n_conflict > 0:
    print(f"\nERROR: {n_conflict} row(s) have multiple values in the fungal_form group.")
    print("Here are the first few problematic rows (only this group's columns):")
    print(df.loc[mask_conflict, present].head())
    raise ValueError("Aborting: conflicting fungal_form values found.")

# --- 5. Coalesce: take the first non-null value left-to-right into canonical 'fungal_form' ---
df["fungal_form"] = df[present].bfill(axis=1).iloc[:, 0]

# --- 6. Drop all non-canonical source columns except 'fungal_form' ---
cols_to_drop = [c for c in present if c != "fungal_form"]
df = df.drop(columns=cols_to_drop)

print("\nFinal 'fungal_form' column created from:")
print(present)
print("\nColumns after fungal_form merge:")
print(list(df.columns))
print(f"New column count: {df.shape[1]}")

# --- 7. Save cleaned file ---
df.to_csv(output_path, index=False)
print(f"\n‚úÖ Saved dataset with merged fungal_form to:\n{output_path}")

import pandas as pd
import os

# If you already have combined_df from before, you can skip this part.
# Otherwise, uncomment and set folder_path to where your combined file lives:

folder_path = "/content/drive/Shareddrives/Danika-work4/Textile/"
combined_path = os.path.join(folder_path, "DeepMyco_BigDataset_fungal_merged.csv")
combined_df = pd.read_csv(combined_path)

# 1. How many rows have at least one missing value?
rows_with_missing = combined_df.isna().any(axis=1).sum()
total_rows = len(combined_df)

print(f"Total rows: {total_rows}")
print(f"Rows with at least one missing value: {rows_with_missing}")
print(f"Percentage of rows with missing values: {rows_with_missing / total_rows * 100:.2f}%")

# 2. Which columns have the most missing values?
missing_per_column = combined_df.isna().sum().sort_values(ascending=False)

print("\nMissing values per column (sorted):")
print(missing_per_column)

# Optional: also show percentage of missing values per column
missing_pct_per_column = (combined_df.isna().mean() * 100).sort_values(ascending=False)

print("\nPercentage of missing values per column (sorted):")
print(missing_pct_per_column)